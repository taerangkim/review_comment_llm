{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3747ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "\n",
    "# SENT_MODEL = 'skt/kobert-base-v1'\n",
    "SENT_MODEL = '/kobert-nsmc-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110af395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# 일반적인 토크나이저 사용시\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(SENT_MODEL)\n",
    "tokenizer.encode(\"한국어 모델을 공유합니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c613fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KoBERT 모델을 라이브러리로 로드\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(SENT_MODEL)\n",
    "text = \"한국어 모델을 공유합니다.\"\n",
    "inputs = tokenizer.batch_encode_plus([text])\n",
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "            attention_mask = torch.tensor(inputs['attention_mask']))\n",
    "\n",
    "out.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5c123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v2-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_id : 1 \n",
      "label : 긍정 \n",
      "confidence: 0.5081313252449036 \n",
      "probs : [0.49186864495277405, 0.5081313252449036]\n"
     ]
    }
   ],
   "source": [
    "# KoBERT 모델을 라이브러리로 로드\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "LABEL_MAP = {0:\"부정\", 1:\"긍정\"}\n",
    "SENT_MODEL = 'monologg/koelectra-small-v2-discriminator'\n",
    "tokenizer = AutoTokenizer.from_pretrained(SENT_MODEL, use_fast=False)\n",
    "\n",
    "model =  AutoModelForSequenceClassification.from_pretrained(SENT_MODEL)\n",
    "model.eval()\n",
    "# text = \"한국어 모델을 공유합니다.\"\n",
    "text = \"음식이 다 식어서 맛이 없었어요\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0].tolist()\n",
    "    pred = int(torch.argmax(logits, dim=-1).item())\n",
    "\n",
    "print(\n",
    "    f'label_id : {pred} \\n'\n",
    "    f'label : {LABEL_MAP.get(pred, \"unknown\")} \\n'\n",
    "    f'confidence: {float(max(probs))} \\n'\n",
    "    f'probs : {probs}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfafb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
